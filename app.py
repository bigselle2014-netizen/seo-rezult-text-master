import streamlit as st
import requests, sqlite3, os, time, re
from dotenv import load_dotenv
from datetime import datetime
from docx import Document
from collections import Counter

# === –ó–ê–ì–†–£–ó–ö–ê –ö–õ–Æ–ß–ï–ô ===
load_dotenv()
API_KEY = os.getenv("PPLX_API_KEY")
TEXT_RU_KEY = os.getenv("TEXT_RU_KEY")

# === –ë–ê–ó–ê –î–ê–ù–ù–´–• ===
DB_PATH = "database.db"
conn = sqlite3.connect(DB_PATH)
conn.execute("""CREATE TABLE IF NOT EXISTS history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    date TEXT,
    topic TEXT,
    symbols INTEGER,
    lsi_count INTEGER,
    text TEXT
)""")
conn.commit()

# === –ù–ê–°–¢–†–û–ô–ö–ò –°–¢–†–ê–ù–ò–¶–´ ===
st.set_page_config(page_title="SEO Rezult Text Master", layout="wide")
st.title("üöÄ SEO Rezult Text Master v4.0")
st.caption("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä SEO-—Ç–µ–∫—Å—Ç–æ–≤ —Å LSI-–∞–Ω–∞–ª–∏–∑–æ–º, –ø—Ä–æ–≤–µ—Ä–∫–æ–π —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∏ SEO-–æ—Ü–µ–Ω–∫–æ–π")

# === –§–û–†–ú–ê –í–í–û–î–ê ===
with st.form("input_form"):
    topic = st.text_input("–¢–µ–º–∞—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞")
    site = st.text_input("–°–∞–π—Ç –∫–ª–∏–µ–Ω—Ç–∞")
    competitors = st.text_area("–°—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–ø–æ –æ–¥–Ω–æ–π –≤ —Å—Ç—Ä–æ–∫–µ)")
    lsi_words = st.text_area("–°–ø–∏—Å–æ–∫ LSI-—Å–ª–æ–≤ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    ngrams = st.text_area("–°–ø–∏—Å–æ–∫ n-–≥—Ä–∞–º–º (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)")
    banned = st.text_area("–ó–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    keywords = st.text_area("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    symbols = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤", value=10000, step=500)
    submitted = st.form_submit_button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç")

# === –§–£–ù–ö–¶–ò–ò ===
def perplexity_generate(prompt: str):
    headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": "sonar-pro",
        "messages": [{"role": "user", "content": [{"type": "text", "text": prompt}]}],
        "max_output_tokens": 2500,
    }
    r = requests.post("https://api.perplexity.ai/chat/completions", json=payload, headers=headers)
    if not r.ok:
        st.error(f"–û—à–∏–±–∫–∞ Perplexity ({r.status_code}): {r.text}")
        r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

def build_prompt(topic, site, competitors, lsi, ngrams, banned, keys, symbols):
    return f"""
–¢—ã –æ–ø—ã—Ç–Ω—ã–π SEO-–∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä –∏ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç.
–ù–∞–ø–∏—à–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π SEO-—Ç–µ–∫—Å—Ç –Ω–∞ —Ç–µ–º—É: {topic}.
–°–∞–π—Ç –∫–ª–∏–µ–Ω—Ç–∞: {site}.
–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã: {competitors}.
–ò—Å–ø–æ–ª—å–∑—É–π –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keys}.
–ò—Å–ø–æ–ª—å–∑—É–π LSI-—Ñ—Ä–∞–∑—ã: {lsi}.
–ò–∑–±–µ–≥–∞–π —Å–ª–æ–≤: {banned}.
–î–æ–±–∞–≤—å —Ñ—Ä–∞–∑—ã: {ngrams}.
–î–ª–∏–Ω–∞ ‚âà {symbols} —Å–∏–º–≤–æ–ª–æ–≤.
–ü–∏—à–∏ –∂–∏–≤—ã–º —è–∑—ã–∫–æ–º, –±–µ–∑ —à–∞–±–ª–æ–Ω–æ–≤ –∏ ¬´–ò–ò-—Ç–æ–Ω–∞¬ª.
"""

def check_missing_lsi(text, lsi_list):
    return [w for w in lsi_list if w.lower() not in text.lower()]

def save_history(topic, symbols, lsi_count, text):
    conn.execute(
        "INSERT INTO history (date,topic,symbols,lsi_count,text) VALUES (?,?,?,?,?)",
        (datetime.now().strftime("%Y-%m-%d %H:%M:%S"), topic, symbols, lsi_count, text),
    )
    conn.commit()

def export_docx(text, filename="seo_text.docx"):
    doc = Document(); doc.add_paragraph(text); doc.save(filename)
    with open(filename, "rb") as f:
        st.download_button("üì• –°–∫–∞—á–∞—Ç—å DOCX", f, file_name=filename)

# === –ü–†–û–í–ï–†–ö–ê –£–ù–ò–ö–ê–õ–¨–ù–û–°–¢–ò ===
def check_text_unique(text):
    if not TEXT_RU_KEY:
        st.warning("üîë –ö–ª—é—á TEXT_RU_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
        return None
    st.info("üßæ –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Text.ru‚Ä¶")
    payload = {"userkey": TEXT_RU_KEY, "text": text, "jsonvisible": "detail"}
    resp = requests.post("https://api.text.ru/post", data=payload).json()
    uid = resp.get("text_uid")
    if not uid:
        st.error("–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Text.ru")
        return None
    for _ in range(20):
        time.sleep(5)
        check = requests.post("https://api.text.ru/post", data={"userkey": TEXT_RU_KEY, "uid": uid}).json()
        if "text_unique" in check:
            return check["text_unique"]
    return None

# === –ê–ù–ê–õ–ò–ó –ï–°–¢–ï–°–¢–í–ï–ù–ù–û–°–¢–ò ===
def analyze_naturalness(text):
    sentences = re.split(r'[.!?]', text)
    avg_len = sum(len(s.split()) for s in sentences if s.strip()) / max(1, len(sentences))
    words = re.findall(r'\w+', text.lower())
    common = Counter(words).most_common(10)
    repeat_score = sum(c for _, c in common[:10]) / max(1, len(words))
    return {
        "avg_sentence_len": round(avg_len, 1),
        "top_words": common[:10],
        "repeat_score": round(repeat_score * 100, 2)
    }

# === SEO-–û–¶–ï–ù–ö–ê ===
def seo_score(unique, coverage, avg_len, repeat_score):
    score = 0
    if unique and unique >= 90: score += 3
    elif unique and unique >= 80: score += 2
    elif unique: score += 1

    if coverage >= 95: score += 3
    elif coverage >= 80: score += 2
    else: score += 1

    if 10 <= avg_len <= 20: score += 2
    else: score += 1

    if repeat_score <= 3: score += 2
    elif repeat_score <= 5: score += 1

    return round(score, 1)

def seo_feedback(unique, coverage, avg_len, repeat_score):
    tips = []
    if unique < 90:
        tips.append("üîÅ –ü–æ–≤—ã—à–∞–π —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å (–∏—Å–ø–æ–ª—å–∑—É–π —Å–∏–Ω–æ–Ω–∏–º—ã).")
    if coverage < 95:
        tips.append("üß© –î–æ–±–∞–≤—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ LSI-—Ñ—Ä–∞–∑—ã.")
    if avg_len > 20:
        tips.append("‚úÇÔ∏è –†–∞–∑–±–µ–π –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–¥–æ 20 —Å–ª–æ–≤).")
    if repeat_score > 3:
        tips.append("üß† –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑—å –ª–µ–∫—Å–∏–∫—É, —Å–Ω–∏–∑—å –ø–æ–≤—Ç–æ—Ä—ã.")
    if not tips:
        tips.append("üéØ –û—Ç–ª–∏—á–Ω—ã–π SEO-—Ç–µ–∫—Å—Ç! –ú–æ–∂–Ω–æ –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å.")
    return tips

# === –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–° ===
if submitted:
    lsi_list = [w.strip() for w in lsi_words.split(",") if w.strip()]
    total_lsi = len(lsi_list)
    progress_bar = st.progress(0)
    status_text = st.empty()

    with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞‚Ä¶"):
        text = perplexity_generate(build_prompt(topic, site, competitors, lsi_words, ngrams, banned, keywords, symbols))

    iteration = 1
    MAX_LSI_PER_ITER = 30

    while True:
        missing = check_missing_lsi(text, lsi_list)
        done = total_lsi - len(missing)
        coverage = int(done / total_lsi * 100) if total_lsi else 100
        progress_bar.progress(min(coverage, 100))
        status_text.text(f"üß© –ü–æ–∫—Ä—ã—Ç–∏–µ LSI: {done}/{total_lsi} ({coverage}%) | –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}")

        if not missing:
            break

        batch = missing[:MAX_LSI_PER_ITER]
        st.info(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç {len(batch)} LSI-—Å–ª–æ–≤ ‚Üí –¥–æ—Ä–∞–±–∞—Ç—ã–≤–∞—é‚Ä¶")

        fix_prompt = (
            f"–°–ø–∞—Å–∏–±–æ –∑–∞ —Ç–µ–∫—Å—Ç, –Ω–æ —Ç—ã –Ω–µ —É—á—ë–ª {len(batch)} LSI-—Å–ª–æ–≤. "
            f"–ü—Ä–æ—à—É –¥–æ–±–∞–≤–∏—Ç—å –∞–±–∑–∞—Ü—ã —Å —ç—Ç–∏–º–∏ —Å–ª–æ–≤–∞–º–∏: {', '.join(batch)}"
        )

        try:
            addition = perplexity_generate(fix_prompt + "\n\n–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\n" + text)
            text += "\n" + addition
            iteration += 1
            time.sleep(1)
        except Exception as e:
            st.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ—Ä–∞–±–æ—Ç–∫–µ: {e}")
            break

    progress_bar.progress(100)
    status_text.text("‚úÖ –í—Å–µ LSI-—Å–ª–æ–≤–∞ —É—á—Ç–µ–Ω—ã. –¢–µ–∫—Å—Ç –≥–æ—Ç–æ–≤!")

    st.success("‚úÖ –¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
    st.text_area("–†–µ–∑—É–ª—å—Ç–∞—Ç", text, height=400)
    export_docx(text)
    save_history(topic, symbols, total_lsi, text)

    # --- –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    unique = check_text_unique(text)
    if unique:
        st.success(f"üßæ –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {unique}%")

    # --- –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
    stats = analyze_naturalness(text)
    st.subheader("üß† –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏")
    st.write(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {stats['avg_sentence_len']} —Å–ª–æ–≤")
    st.write(f"–ü–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç—å —Ç–æ–ø-—Å–ª–æ–≤: {stats['repeat_score']}%")
    st.write("–ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞:")
    st.json(stats['top_words'])

    # --- SEO-–û—Ü–µ–Ω–∫–∞
    st.subheader("üìä SEO-–û—Ü–µ–Ω–∫–∞ —Ç–µ–∫—Å—Ç–∞")
    coverage = 100
    final_score = seo_score(unique or 100, coverage, stats['avg_sentence_len'], stats['repeat_score'])
    st.metric("–û–±—â–∞—è SEO-–æ—Ü–µ–Ω–∫–∞", f"{final_score} / 10")
    st.metric("–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å", f"{unique}%")
    st.metric("LSI-–ø–æ–∫—Ä—ã—Ç–∏–µ", f"{coverage}%")
    st.metric("–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", f"{stats['avg_sentence_len']} —Å–ª–æ–≤")
    st.metric("–ü–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç—å —Ç–æ–ø-—Å–ª–æ–≤", f"{stats['repeat_score']}%")

    st.subheader("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
    for tip in seo_feedback(unique or 100, coverage, stats['avg_sentence_len'], stats['repeat_score']):
        st.write(tip)

# === –ò–°–¢–û–†–ò–Ø ===
st.subheader("üìö –ò—Å—Ç–æ—Ä–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–π")
rows = conn.execute("SELECT id,date,topic,symbols,lsi_count FROM history ORDER BY id DESC").fetchall()
if rows:
    for row in rows:
        with st.expander(f"{row[1]} ‚Äî {row[2]}"):
            st.write(f"–û–±—ä—ë–º: {row[3]} —Å–∏–º–≤–æ–ª–æ–≤, LSI: {row[4]}")
            txt = conn.execute("SELECT text FROM history WHERE id=?", (row[0],)).fetchone()[0]
            st.text_area("–¢–µ–∫—Å—Ç", txt, height=200)
else:
    st.info("–ò—Å—Ç–æ—Ä–∏—è –ø–æ–∫–∞ –ø—É—Å—Ç–∞.")
