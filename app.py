import streamlit as st
import requests, re, os, numpy as np
from datetime import datetime
from docx import Document
from urllib.parse import quote
from supabase import create_client
from dotenv import load_dotenv
from auth import login_or_register  # –∏–º–ø–æ—Ä—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
st.set_page_config(page_title="SEO Rezult Text Master v6.0", layout="wide")

load_dotenv()
PPLX_API_KEY = os.getenv("PPLX_API_KEY") or st.secrets["PPLX_API_KEY"]
TEXT_RU_KEY = os.getenv("TEXT_RU_KEY") or st.secrets["TEXT_RU_KEY"]
SUPABASE_URL = os.getenv("SUPABASE_URL") or st.secrets["SUPABASE_URL"]
SUPABASE_KEY = os.getenv("SUPABASE_KEY") or st.secrets["SUPABASE_KEY"]
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# === –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è ===
user = login_or_register()
if not user:
    st.stop()

st.title("üöÄ SEO Rezult Text Master v6.0")
st.caption("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä SEO-—Ç–µ–∫—Å—Ç–æ–≤ —Å LSI-–∞–Ω–∞–ª–∏–∑–æ–º, —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å—é –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é")

# --- –§–æ—Ä–º–∞ ---
with st.form("input_form"):
    topic = st.text_input("–¢–µ–º–∞—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞")
    site = st.text_input("–°–∞–π—Ç –∫–ª–∏–µ–Ω—Ç–∞")
    competitors = st.text_area("–°—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–ø–æ –æ–¥–Ω–æ–π –≤ —Å—Ç—Ä–æ–∫–µ)")
    lsi_words = st.text_area("–°–ø–∏—Å–æ–∫ LSI-—Å–ª–æ–≤ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    banned = st.text_area("–ó–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    keywords = st.text_area("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    symbols = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤", value=8000, step=500)
    submitted = st.form_submit_button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç")

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
def perplexity_generate(prompt: str):
    headers = {"Authorization": f"Bearer {PPLX_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": "sonar-pro", "messages": [{"role": "user", "content": [{"type": "text", "text": prompt}]}]}
    r = requests.post("https://api.perplexity.ai/chat/completions", json=payload, headers=headers)
    return r.json()["choices"][0]["message"]["content"]

def build_prompt(topic, site, competitors, lsi, banned, keys, symbols):
    return f"""
–¢—ã –æ–ø—ã—Ç–Ω—ã–π SEO-–∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä.
–ù–∞–ø–∏—à–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π SEO-—Ç–µ–∫—Å—Ç –Ω–∞ —Ç–µ–º—É: {topic}.
–°–∞–π—Ç –∫–ª–∏–µ–Ω—Ç–∞: {site}.
–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã: {competitors}.
–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keys}.
LSI-—Ñ—Ä–∞–∑—ã: {lsi}.
–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ª–æ–≤–∞: {banned}.
–û–±—ä—ë–º ‚âà {symbols} —Å–∏–º–≤–æ–ª–æ–≤.
–ü–∏—à–∏ –∂–∏–≤—ã–º —è–∑—ã–∫–æ–º, –±–µ–∑ —à–∞–±–ª–æ–Ω–æ–≤, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ.
"""

def clean_text(text): return re.sub(r"[#*_>`]+", "", text).strip()
def check_missing_lsi(text, lsi_list): return [w for w in lsi_list if w.lower() not in text.lower()]

def seo_score(text, keywords):
    words = re.findall(r"\w+", text.lower())
    word_count = len(words)
    avg_len = np.mean([len(w) for w in words])
    key_density = sum(text.lower().count(k.lower()) for k in keywords.split(",")) / max(word_count, 1) * 100
    return {"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤": word_count, "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞": round(avg_len, 2), "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π (%)": round(key_density, 2)}

def analyze_humanness(text):
    words = re.findall(r"\w+", text.lower())
    unique_words = len(set(words))
    perplexity = round(np.exp(len(words) / max(unique_words, 1)), 2)
    human_score = max(0, min(100, round(100 - (perplexity / 50) * 20, 1)))
    return {"–ü–µ—Ä–ø–ª–µ–∫—Å–∏—è": perplexity, "–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å (%)": human_score}

def export_docx(text, report, filename="seo_text.docx"):
    doc = Document()
    doc.add_heading("SEO Rezult Text Master ‚Äî –û—Ç—á—ë—Ç", level=1)
    doc.add_paragraph(text)
    doc.add_page_break()
    for k, v in report.items():
        doc.add_paragraph(f"{k}: {v}")
    doc.save(filename)
    with open(filename, "rb") as f:
        st.download_button("üì• –°–∫–∞—á–∞—Ç—å DOCX", f, file_name=filename)

# --- –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å ---
if submitted:
    st.info("‚öôÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...")
    lsi_list = [w.strip() for w in lsi_words.split(",") if w.strip()]
    text = clean_text(perplexity_generate(build_prompt(topic, site, competitors, lsi_words, banned, keywords, symbols)))

    st.text_area("üìù –†–µ–∑—É–ª—å—Ç–∞—Ç", text, height=400)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Supabase
    supabase.table("history").insert({
        "user_id": user.id,
        "topic": topic,
        "symbols": symbols,
        "lsi_count": len(lsi_list),
        "text": text,
        "date": datetime.now().isoformat()
    }).execute()

    report = seo_score(text, keywords)
    st.table(report.items())
    human = analyze_humanness(text)
    st.table(human.items())

    export_docx(text, report)
