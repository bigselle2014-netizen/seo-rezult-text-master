import streamlit as st
import requests, sqlite3, os, re
from dotenv import load_dotenv
from datetime import datetime
from docx import Document
from urllib.parse import quote

load_dotenv()
PPLX_API_KEY = os.getenv("PPLX_API_KEY")
TEXT_RU_KEY = os.getenv("TEXT_RU_KEY")

DB_PATH = "database.db"
conn = sqlite3.connect(DB_PATH)
conn.execute("""CREATE TABLE IF NOT EXISTS history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    date TEXT,
    topic TEXT,
    symbols INTEGER,
    lsi_count INTEGER,
    text TEXT
)""")
conn.commit()

st.set_page_config(page_title="SEO Rezult Text Master v5.0", layout="wide")

st.title("üöÄ SEO Rezult Text Master v5.0")
st.caption("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä SEO-—Ç–µ–∫—Å—Ç–æ–≤ —Å LSI-–∞–Ω–∞–ª–∏–∑–æ–º, –ø—Ä–æ–≤–µ—Ä–∫–æ–π —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∏ SEO-–æ—Ü–µ–Ω–∫–æ–π")

# =================== –í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö ===================
with st.form("input_form"):
    topic = st.text_input("–¢–µ–º–∞—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞")
    site = st.text_input("–°–∞–π—Ç –∫–ª–∏–µ–Ω—Ç–∞")
    competitors = st.text_area("–°—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–ø–æ –æ–¥–Ω–æ–π –≤ —Å—Ç—Ä–æ–∫–µ)")
    lsi_words = st.text_area("–°–ø–∏—Å–æ–∫ LSI-—Å–ª–æ–≤ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    banned = st.text_area("–ó–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    keywords = st.text_area("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    symbols = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤", value=8000, step=500)
    submitted = st.form_submit_button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç")

# =================== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ===================
def perplexity_generate(prompt: str):
    headers = {"Authorization": f"Bearer {PPLX_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": "sonar-pro",
        "messages": [{"role": "user", "content": [{"type": "text", "text": prompt}]}],
        "max_output_tokens": 2000
    }
    r = requests.post("https://api.perplexity.ai/chat/completions", json=payload, headers=headers)
    if not r.ok:
        st.error(f"Perplexity API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É {r.status_code}: {r.text}")
        r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

def build_prompt(topic, site, competitors, lsi, banned, keys, symbols):
    return f"""
–¢—ã –æ–ø—ã—Ç–Ω—ã–π SEO-–∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä. 
–ù–∞–ø–∏—à–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π SEO-—Ç–µ–∫—Å—Ç –Ω–∞ —Ç–µ–º—É: {topic}.
–°–∞–π—Ç –∫–ª–∏–µ–Ω—Ç–∞: {site}.
–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã: {competitors}.
–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keys}.
LSI-—Ñ—Ä–∞–∑—ã: {lsi}.
–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ª–æ–≤–∞: {banned}.
–î–ª–∏–Ω–∞ ‚âà {symbols} —Å–∏–º–≤–æ–ª–æ–≤.
–ü–∏—à–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ, –±–µ–∑ —à–∞–±–ª–æ–Ω–æ–≤ –∏ —Å —Ñ–∞–∫—Ç–∞–º–∏.
"""

def clean_text(text):
    return re.sub(r"[#*_>`]+", "", text).strip()

def check_missing_lsi(text, lsi_list):
    return [w for w in lsi_list if w.lower() not in text.lower()]

def save_history(topic, symbols, lsi_count, text):
    conn.execute("INSERT INTO history (date,topic,symbols,lsi_count,text) VALUES (?,?,?, ?, ?)",
                 (datetime.now().strftime("%Y-%m-%d %H:%M:%S"), topic, symbols, lsi_count, text))
    conn.commit()

def export_docx(text, report, filename="seo_text.docx"):
    doc = Document()
    doc.add_heading("SEO Rezult Text Master", level=1)
    doc.add_paragraph(text)
    doc.add_page_break()
    doc.add_heading("SEO-–æ—Ü–µ–Ω–∫–∞", level=2)
    for k, v in report.items():
        doc.add_paragraph(f"{k}: {v}")
    doc.save(filename)
    with open(filename, "rb") as f:
        st.download_button("üì• –°–∫–∞—á–∞—Ç—å DOCX", f, file_name=filename)

# =================== SEO-–∞–Ω–∞–ª–∏–∑ ===================
def seo_score(text, keywords):
    words = re.findall(r"\w+", text.lower())
    word_count = len(words)
    avg_len = sum(len(w) for w in words) / len(words)
    key_density = sum(text.lower().count(k.lower()) for k in keywords.split(",")) / max(word_count, 1) * 100
    sentences = re.split(r"[.!?]", text)
    avg_sentence_len = sum(len(s.split()) for s in sentences if s.strip()) / max(len(sentences), 1)
    water = len(re.findall(r"\b(–æ—á–µ–Ω—å|—ç—Ç–æ|—Ç–∞–∫–∂–µ|–ø–æ—ç—Ç–æ–º—É|–Ω–∞–ø—Ä–∏–º–µ—Ä)\b", text.lower())) / max(word_count, 1) * 100
    return {
        "–û–±—â–µ–µ –∫–æ–ª-–≤–æ —Å–ª–æ–≤": word_count,
        "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞": round(avg_len, 2),
        "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è": round(avg_sentence_len, 2),
        "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π (%)": round(key_density, 2),
        "–í–æ–¥–Ω–æ—Å—Ç—å (%)": round(water, 2)
    }

# =================== –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å ===================
if submitted:
    st.info("‚öôÔ∏è –≠—Ç–∞–ø 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Perplexity...")
    lsi_list = [w.strip() for w in lsi_words.split(",") if w.strip()]
    text = perplexity_generate(build_prompt(topic, site, competitors, lsi_words, banned, keywords, symbols))
    text = clean_text(text)
    iteration = 1

    progress = st.progress(0)
    while True:
        missing = check_missing_lsi(text, lsi_list)
        if not missing: break
        st.warning(f"–≠—Ç–∞–ø 2: –¥–æ—Ä–∞–±–æ—Ç–∫–∞ LSI ({len(missing)} —Å–ª–æ–≤)...")
        addition = perplexity_generate(f"–î–æ–±–∞–≤—å –∞–±–∑–∞—Ü —Å–æ —Å–ª–æ–≤–∞–º–∏: {', '.join(missing)}.\n\n{text}")
        text += "\n" + clean_text(addition)
        iteration += 1
        progress.progress(min(90, iteration * 20))

    progress.progress(100)
    st.success("‚úÖ –¢–µ–∫—Å—Ç –≥–æ—Ç–æ–≤!")
    st.text_area("–†–µ–∑—É–ª—å—Ç–∞—Ç", text, height=400)

    # === –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ ===
    st.info("üîé –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Text.ru API...")
    r = requests.post("https://api.text.ru/post", data={"text": text, "userkey": TEXT_RU_KEY})
    if r.ok:
        res = requests.get("https://api.text.ru/post", params={"uid": r.json()["text_uid"], "userkey": TEXT_RU_KEY}).json()
        uniq = res.get("text_unique", "?")
        st.write(f"**–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å:** {uniq}%")
    else:
        st.warning("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏.")

    # === SEO-–æ—Ü–µ–Ω–∫–∞ ===
    st.info("üìä –ê–Ω–∞–ª–∏–∑ SEO-–ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π...")
    report = seo_score(text, keywords)
    st.table(report.items())

    # === –ö–Ω–æ–ø–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –ò–ò ===
    st.markdown(f"[üß† –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –ò–ò –∏ —á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç—å](https://aidetectorwriter.com/ru/?text={quote(text)})")

    export_docx(text, report)
    save_history(topic, symbols, len(lsi_list), text)
