import streamlit as st
import requests, sqlite3, os, re, numpy as np
from dotenv import load_dotenv
from datetime import datetime
from docx import Document
from urllib.parse import quote

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è ---
load_dotenv()
PPLX_API_KEY = os.getenv("PPLX_API_KEY")
TEXT_RU_KEY = os.getenv("TEXT_RU_KEY")

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã ---
DB_PATH = "database.db"
conn = sqlite3.connect(DB_PATH)
conn.execute("""CREATE TABLE IF NOT EXISTS history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    date TEXT,
    topic TEXT,
    symbols INTEGER,
    lsi_count INTEGER,
    text TEXT
)""")
conn.commit()

st.set_page_config(page_title="SEO Rezult Text Master v6.0", layout="wide")
st.title("üöÄ SEO Rezult Text Master v6.0")
st.caption("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä SEO-—Ç–µ–∫—Å—Ç–æ–≤ —Å LSI-–∞–Ω–∞–ª–∏–∑–æ–º, –ø—Ä–æ–≤–µ—Ä–∫–æ–π —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏, SEO-–æ—Ü–µ–Ω–∫–æ–π –∏ –∞–Ω–∞–ª–∏–∑–æ–º –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏")

# --- –§–æ—Ä–º–∞ –≤–≤–æ–¥–∞ ---
with st.form("input_form"):
    topic = st.text_input("–¢–µ–º–∞—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞")
    site = st.text_input("–°–∞–π—Ç –∫–ª–∏–µ–Ω—Ç–∞")
    competitors = st.text_area("–°—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–ø–æ –æ–¥–Ω–æ–π –≤ —Å—Ç—Ä–æ–∫–µ)")
    lsi_words = st.text_area("–°–ø–∏—Å–æ–∫ LSI-—Å–ª–æ–≤ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    banned = st.text_area("–ó–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    keywords = st.text_area("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
    symbols = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤", value=8000, step=500)
    submitted = st.form_submit_button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç")

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
def perplexity_generate(prompt: str):
    headers = {"Authorization": f"Bearer {PPLX_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": "sonar-pro",
        "messages": [{"role": "user", "content": [{"type": "text", "text": prompt}]}],
        "max_output_tokens": 2000
    }
    r = requests.post("https://api.perplexity.ai/chat/completions", json=payload, headers=headers)
    if not r.ok:
        st.error(f"Perplexity API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É {r.status_code}: {r.text}")
        r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

def build_prompt(topic, site, competitors, lsi, banned, keys, symbols):
    return f"""
–¢—ã –æ–ø—ã—Ç–Ω—ã–π SEO-–∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä.
–ù–∞–ø–∏—à–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π SEO-—Ç–µ–∫—Å—Ç –Ω–∞ —Ç–µ–º—É: {topic}.
–°–∞–π—Ç –∫–ª–∏–µ–Ω—Ç–∞: {site}.
–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã: {competitors}.
–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keys}.
LSI-—Ñ—Ä–∞–∑—ã: {lsi}.
–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ª–æ–≤–∞: {banned}.
–û–±—ä—ë–º ‚âà {symbols} —Å–∏–º–≤–æ–ª–æ–≤.
–ü–∏—à–∏ –∂–∏–≤—ã–º —è–∑—ã–∫–æ–º, –±–µ–∑ —à–∞–±–ª–æ–Ω–æ–≤, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ.
"""

def clean_text(text):
    return re.sub(r"[#*_>`]+", "", text).strip()

def check_missing_lsi(text, lsi_list):
    return [w for w in lsi_list if w.lower() not in text.lower()]

def save_history(topic, symbols, lsi_count, text):
    conn.execute("INSERT INTO history (date,topic,symbols,lsi_count,text) VALUES (?,?,?,?,?)",
                 (datetime.now().strftime("%Y-%m-%d %H:%M:%S"), topic, symbols, lsi_count, text))
    conn.commit()

def export_docx(text, report, human_report, filename="seo_text.docx"):
    doc = Document()
    doc.add_heading("SEO Rezult Text Master ‚Äî –û—Ç—á—ë—Ç", level=1)
    doc.add_paragraph(text)
    doc.add_page_break()
    doc.add_heading("üìä SEO-–∞–Ω–∞–ª–∏–∑", level=2)
    for k, v in report.items():
        doc.add_paragraph(f"{k}: {v}")
    doc.add_heading("üß† –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏", level=2)
    for k, v in human_report.items():
        doc.add_paragraph(f"{k}: {v}")
    doc.save(filename)
    with open(filename, "rb") as f:
        st.download_button("üì• –°–∫–∞—á–∞—Ç—å DOCX-–æ—Ç—á—ë—Ç", f, file_name=filename)

# --- SEO-–∞–Ω–∞–ª–∏–∑ ---
def seo_score(text, keywords):
    words = re.findall(r"\w+", text.lower())
    word_count = len(words)
    avg_len = sum(len(w) for w in words) / len(words)
    key_density = sum(text.lower().count(k.lower()) for k in keywords.split(",")) / max(word_count, 1) * 100
    sentences = re.split(r"[.!?]", text)
    avg_sentence_len = sum(len(s.split()) for s in sentences if s.strip()) / max(len(sentences), 1)
    water = len(re.findall(r"\b(–æ—á–µ–Ω—å|—ç—Ç–æ|—Ç–∞–∫–∂–µ|–ø–æ—ç—Ç–æ–º—É|–Ω–∞–ø—Ä–∏–º–µ—Ä|–≤ —Ü–µ–ª–æ–º|—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)\b", text.lower())) / max(word_count, 1) * 100
    return {
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤": word_count,
        "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞": round(avg_len, 2),
        "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è": round(avg_sentence_len, 2),
        "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π (%)": round(key_density, 2),
        "–í–æ–¥–Ω–æ—Å—Ç—å (%)": round(water, 2)
    }

# --- –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ ---
def analyze_humanness(text):
    sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]
    words = re.findall(r'\w+', text.lower())

    unique_words = len(set(words))
    perplexity = round(np.exp(len(words) / max(unique_words, 1)), 2)

    sentence_lengths = [len(s.split()) for s in sentences]
    burstiness = round(np.std(sentence_lengths) / (np.mean(sentence_lengths) + 1e-5), 2)

    bigrams = [f"{words[i]} {words[i+1]}" for i in range(len(words)-1)]
    repeats = len(bigrams) - len(set(bigrams))
    repeat_ratio = round(repeats / max(len(bigrams), 1) * 100, 2)

    human_score = 100 - ((perplexity / 50) * 20 + (repeat_ratio / 2) - (burstiness * 10))
    human_score = max(0, min(100, round(human_score, 1)))

    return {
        "–ü–µ—Ä–ø–ª–µ–∫—Å–∏—è (–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å)": perplexity,
        "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (Burstiness)": burstiness,
        "–ü–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç—å —Ñ—Ä–∞–∑ (%)": repeat_ratio,
        "–û—Ü–µ–Ω–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ (%)": human_score
    }

# --- –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å ---
if submitted:
    st.info("‚öôÔ∏è –≠—Ç–∞–ø 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Perplexity...")
    lsi_list = [w.strip() for w in lsi_words.split(",") if w.strip()]
    text = perplexity_generate(build_prompt(topic, site, competitors, lsi_words, banned, keywords, symbols))
    text = clean_text(text)
    iteration = 1

    progress = st.progress(0)
    while True:
        missing = check_missing_lsi(text, lsi_list)
        if not missing: break
        st.warning(f"–≠—Ç–∞–ø 2: –¥–æ—Ä–∞–±–æ—Ç–∫–∞ LSI ({len(missing)} —Å–ª–æ–≤)...")
        addition = perplexity_generate(f"–î–æ–±–∞–≤—å –∞–±–∑–∞—Ü —Å–æ —Å–ª–æ–≤–∞–º–∏: {', '.join(missing)}.\n\n{text}")
        text += "\n" + clean_text(addition)
        iteration += 1
        progress.progress(min(90, iteration * 20))

    progress.progress(100)
    st.success("‚úÖ –¢–µ–∫—Å—Ç –≥–æ—Ç–æ–≤!")
    st.text_area("–†–µ–∑—É–ª—å—Ç–∞—Ç", text, height=400)

    # === –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ ===
    st.info("üîé –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Text.ru API...")
    r = requests.post("https://api.text.ru/post", data={"text": text, "userkey": TEXT_RU_KEY})
    if r.ok:
        res = requests.get("https://api.text.ru/post", params={"uid": r.json()["text_uid"], "userkey": TEXT_RU_KEY}).json()
        uniq = res.get("text_unique", "?")
        st.write(f"**–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å:** {uniq}%")
    else:
        st.warning("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏.")

    # === SEO-–æ—Ü–µ–Ω–∫–∞ ===
    st.info("üìä SEO-–∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞...")
    report = seo_score(text, keywords)
    st.table(report.items())

    # === –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ ===
    st.info("üß† –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞...")
    human_report = analyze_humanness(text)
    st.table(human_report.items())
    score = human_report["–û—Ü–µ–Ω–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ (%)"]

    if score >= 85:
        st.success(f"‚úÖ –¢–µ–∫—Å—Ç –≤—ã–≥–ª—è–¥–∏—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º ({score}%) ‚Äî –Ω–∞–ø–∏—Å–∞–Ω –∂–∏–≤–æ –∏ –ø–æ-—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏.")
    elif score >= 70:
        st.info(f"üü¢ –¢–µ–∫—Å—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π ({score}%) ‚Äî –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —à–∞–±–ª–æ–Ω–Ω–æ—Å—Ç–∏.")
    elif score >= 50:
        st.warning(f"üü† –¢–µ–∫—Å—Ç –≤—ã–≥–ª—è–¥–∏—Ç –æ—Ç—á–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω—ã–º ({score}%) ‚Äî –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç–∏–ª—å.")
    else:
        st.error(f"üî¥ –¢–µ–∫—Å—Ç –ø–æ—Ö–æ–∂ –Ω–∞ –ò–ò ({score}%) ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–∏–ª—å–Ω–∞—è —Ä–µ–¥–∞–∫—Ç—É—Ä–∞.")

    # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ò–ò (–≤–Ω–µ—à–Ω—è—è –∫–Ω–æ–ø–∫–∞) ===
    st.markdown(f"[üß© –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ —Å–∞–π—Ç–µ AI Detector](https://aidetectorwriter.com/ru/?text={quote(text)})")

    export_docx(text, report, human_report)
    save_history(topic, symbols, len(lsi_list), text)
