import streamlit as st
import requests, re, os, numpy as np
from datetime import datetime
from docx import Document
from urllib.parse import quote
from supabase import create_client, Client
from dotenv import load_dotenv

# =========================
# ‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ê –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–ô
# =========================
load_dotenv()
PPLX_API_KEY = os.getenv("PPLX_API_KEY") or st.secrets.get("PPLX_API_KEY")
TEXT_RU_KEY = os.getenv("TEXT_RU_KEY") or st.secrets.get("TEXT_RU_KEY")

SUPABASE_URL = st.secrets["SUPABASE_URL"]
SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

st.set_page_config(page_title="SEO Rezult Text Master v6.0", layout="wide")

# =========================
# üîê –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø
# =========================
if "user" not in st.session_state:
    st.session_state.user = None

with st.sidebar:
    st.header("üîê –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è")

    if st.session_state.user:
        st.success(f"–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å, {st.session_state.user['email']}!")
        if st.button("–í—ã–π—Ç–∏"):
            st.session_state.user = None
            st.experimental_rerun()
    else:
        mode = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:", ["–í—Ö–æ–¥", "–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è"])
        email = st.text_input("Email")
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password")

        if st.button("–í–æ–π—Ç–∏"):
            try:
                if mode == "–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è":
                    res = supabase.auth.sign_up({"email": email, "password": password})
                else:
                    res = supabase.auth.sign_in_with_password({"email": email, "password": password})

                if res.user:
                    st.session_state.user = {"email": email, "id": res.user.id}
                    st.experimental_rerun()
                else:
                    st.error("–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.")
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –≤—Ö–æ–¥–∞: {e}")

# =========================
# üß† –û–°–ù–û–í–ù–û–ô –ò–ù–¢–ï–†–§–ï–ô–°
# =========================
if st.session_state.user:
    st.title("üöÄ SEO Rezult Text Master v6.0")
    st.caption("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä SEO-—Ç–µ–∫—Å—Ç–æ–≤ —Å LSI-–∞–Ω–∞–ª–∏–∑–æ–º, —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å—é –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é")

    tabs = st.tabs(["üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è", "üìÇ –ú–æ–∏ —Ç–µ–∫—Å—Ç—ã"])

    # -----------------------------------------------------
    # –í–∫–ª–∞–¥–∫–∞ 1 ‚Äî –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    # -----------------------------------------------------
    with tabs[0]:
        with st.form("input_form"):
            topic = st.text_input("–¢–µ–º–∞—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞")
            site = st.text_input("–°–∞–π—Ç –∫–ª–∏–µ–Ω—Ç–∞")
            competitors = st.text_area("–°—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–ø–æ –æ–¥–Ω–æ–π –≤ —Å—Ç—Ä–æ–∫–µ)")
            lsi_words = st.text_area("–°–ø–∏—Å–æ–∫ LSI-—Å–ª–æ–≤ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
            banned = st.text_area("–ó–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
            keywords = st.text_area("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)")
            symbols = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤", value=8000, step=500)
            submitted = st.form_submit_button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç")

        # =======================
        # üîß –§–£–ù–ö–¶–ò–ò
        # =======================
        def perplexity_generate(prompt: str):
            headers = {"Authorization": f"Bearer {PPLX_API_KEY}", "Content-Type": "application/json"}
            payload = {
                "model": "sonar-pro",
                "messages": [{"role": "user", "content": [{"type": "text", "text": prompt}]}],
                "max_output_tokens": 2000
            }
            r = requests.post("https://api.perplexity.ai/chat/completions", json=payload, headers=headers)
            if not r.ok:
                st.error(f"Perplexity API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É {r.status_code}: {r.text}")
                r.raise_for_status()
            return r.json()["choices"][0]["message"]["content"]

        def build_prompt(topic, site, competitors, lsi, banned, keys, symbols):
            return f"""
–¢—ã –æ–ø—ã—Ç–Ω—ã–π SEO-–∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä.
–ù–∞–ø–∏—à–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π SEO-—Ç–µ–∫—Å—Ç –Ω–∞ —Ç–µ–º—É: {topic}.
–°–∞–π—Ç –∫–ª–∏–µ–Ω—Ç–∞: {site}.
–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã: {competitors}.
–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keys}.
LSI-—Ñ—Ä–∞–∑—ã: {lsi}.
–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ª–æ–≤–∞: {banned}.
–û–±—ä—ë–º ‚âà {symbols} —Å–∏–º–≤–æ–ª–æ–≤.
–ü–∏—à–∏ –∂–∏–≤—ã–º —è–∑—ã–∫–æ–º, –±–µ–∑ —à–∞–±–ª–æ–Ω–æ–≤, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ.
"""

        def clean_text(text):
            return re.sub(r"[#*_>`]+", "", text).strip()

        def check_missing_lsi(text, lsi_list):
            return [w for w in lsi_list if w.lower() not in text.lower()]

        def export_docx(text, report, human_report, filename="seo_text.docx"):
            doc = Document()
            doc.add_heading("SEO Rezult Text Master ‚Äî –û—Ç—á—ë—Ç", level=1)
            doc.add_paragraph(text)
            doc.add_page_break()
            doc.add_heading("üìä SEO-–∞–Ω–∞–ª–∏–∑", level=2)
            for k, v in report.items():
                doc.add_paragraph(f"{k}: {v}")
            doc.add_heading("üß† –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏", level=2)
            for k, v in human_report.items():
                doc.add_paragraph(f"{k}: {v}")
            doc.save(filename)
            with open(filename, "rb") as f:
                st.download_button("üì• –°–∫–∞—á–∞—Ç—å DOCX-–æ—Ç—á—ë—Ç", f, file_name=filename)

        def seo_score(text, keywords):
            words = re.findall(r"\w+", text.lower())
            word_count = len(words)
            avg_len = sum(len(w) for w in words) / len(words)
            key_density = sum(text.lower().count(k.lower()) for k in keywords.split(",")) / max(word_count, 1) * 100
            sentences = re.split(r"[.!?]", text)
            avg_sentence_len = sum(len(s.split()) for s in sentences if s.strip()) / max(len(sentences), 1)
            water = len(re.findall(r"\b(–æ—á–µ–Ω—å|—ç—Ç–æ|—Ç–∞–∫–∂–µ|–ø–æ—ç—Ç–æ–º—É|–Ω–∞–ø—Ä–∏–º–µ—Ä|–≤ —Ü–µ–ª–æ–º|—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)\b", text.lower())) / max(word_count, 1) * 100
            return {
                "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤": word_count,
                "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞": round(avg_len, 2),
                "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è": round(avg_sentence_len, 2),
                "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π (%)": round(key_density, 2),
                "–í–æ–¥–Ω–æ—Å—Ç—å (%)": round(water, 2)
            }

        def analyze_humanness(text):
            sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]
            words = re.findall(r'\w+', text.lower())
            unique_words = len(set(words))
            perplexity = round(np.exp(len(words) / max(unique_words, 1)), 2)
            sentence_lengths = [len(s.split()) for s in sentences]
            burstiness = round(np.std(sentence_lengths) / (np.mean(sentence_lengths) + 1e-5), 2)
            bigrams = [f"{words[i]} {words[i+1]}" for i in range(len(words)-1)]
            repeats = len(bigrams) - len(set(bigrams))
            repeat_ratio = round(repeats / max(len(bigrams), 1) * 100, 2)
            human_score = 100 - ((perplexity / 50) * 20 + (repeat_ratio / 2) - (burstiness * 10))
            human_score = max(0, min(100, round(human_score, 1)))
            return {
                "–ü–µ—Ä–ø–ª–µ–∫—Å–∏—è (–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å)": perplexity,
                "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (Burstiness)": burstiness,
                "–ü–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç—å —Ñ—Ä–∞–∑ (%)": repeat_ratio,
                "–û—Ü–µ–Ω–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ (%)": human_score
            }

        # =======================
        # üöÄ –ì–ï–ù–ï–†–ê–¶–ò–Ø
        # =======================
        if submitted:
            st.info("‚öôÔ∏è –≠—Ç–∞–ø 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Perplexity...")
            lsi_list = [w.strip() for w in lsi_words.split(",") if w.strip()]
            text = perplexity_generate(build_prompt(topic, site, competitors, lsi_words, banned, keywords, symbols))
            text = clean_text(text)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ LSI
            iteration = 1
            progress = st.progress(0)
            while True:
                missing = check_missing_lsi(text, lsi_list)
                if not missing:
                    break
                st.warning(f"–≠—Ç–∞–ø 2: –¥–æ—Ä–∞–±–æ—Ç–∫–∞ LSI ({len(missing)} —Å–ª–æ–≤)...")
                addition = perplexity_generate(f"–î–æ–±–∞–≤—å –∞–±–∑–∞—Ü —Å–æ —Å–ª–æ–≤–∞–º–∏: {', '.join(missing)}.\n\n{text}")
                text += "\n" + clean_text(addition)
                iteration += 1
                progress.progress(min(90, iteration * 20))

            progress.progress(100)
            st.success("‚úÖ –¢–µ–∫—Å—Ç –≥–æ—Ç–æ–≤!")
            st.text_area("–†–µ–∑—É–ª—å—Ç–∞—Ç", text, height=400)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            st.info("üîé –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Text.ru API...")
            r = requests.post("https://api.text.ru/post", data={"text": text, "userkey": TEXT_RU_KEY})
            if r.ok:
                res = requests.get("https://api.text.ru/post", params={"uid": r.json()["text_uid"], "userkey": TEXT_RU_KEY}).json()
                uniq = res.get("text_unique", "?")
                st.write(f"**–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å:** {uniq}%")
            else:
                st.warning("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏.")

            # SEO-–∞–Ω–∞–ª–∏–∑
            st.info("üìä SEO-–∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞...")
            report = seo_score(text, keywords)
            st.table(report.items())

            # –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
            st.info("üß† –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞...")
            human_report = analyze_humanness(text)
            st.table(human_report.items())
            score = human_report["–û—Ü–µ–Ω–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ (%)"]

            if score >= 85:
                st.success(f"‚úÖ –¢–µ–∫—Å—Ç –≤—ã–≥–ª—è–¥–∏—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º ({score}%)")
            elif score >= 70:
                st.info(f"üü¢ –í —Ü–µ–ª–æ–º –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π ({score}%)")
            elif score >= 50:
                st.warning(f"üü† –ß–∞—Å—Ç–∏—á–Ω–æ –º–∞—à–∏–Ω–Ω—ã–π ({score}%)")
            else:
                st.error(f"üî¥ –ü–æ—Ö–æ–∂ –Ω–∞ –ò–ò ({score}%) ‚Äî –¥–æ—Ä–∞–±–æ—Ç–∞–π.")

            st.markdown(f"[üß© –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ —Å–∞–π—Ç–µ AI Detector](https://aidetectorwriter.com/ru/?text={quote(text)})")

            export_docx(text, report, human_report)
            supabase.table("history").insert({
                "user_id": st.session_state.user["id"],
                "date": datetime.now().isoformat(),
                "topic": topic,
                "symbols": symbols,
                "lsi_count": len(lsi_list),
                "text": text
            }).execute()

    # -----------------------------------------------------
    # –í–∫–ª–∞–¥–∫–∞ 2 ‚Äî –ò—Å—Ç–æ—Ä–∏—è —Ç–µ–∫—Å—Ç–æ–≤
    # -----------------------------------------------------
    with tabs[1]:
        st.subheader("üìÇ –ò—Å—Ç–æ—Ä–∏—è —Ç–µ–∫—Å—Ç–æ–≤")
        user_id = st.session_state.user["id"]
        data = supabase.table("history").select("*").eq("user_id", user_id).order("date", desc=True).execute()

        if data.data:
            for row in data.data:
                with st.expander(f"{row['topic']} ‚Äî {row['date']}"):
                    st.write(row["text"][:400] + "...")
                    col1, col2 = st.columns([0.8, 0.2])
                    with col1:
                        st.caption(f"–°–∏–º–≤–æ–ª–æ–≤: {row['symbols']}, LSI: {row['lsi_count']}")
                    with col2:
                        if st.button("üóë –£–¥–∞–ª–∏—Ç—å", key=row["id"]):
                            supabase.table("history").delete().eq("id", row["id"]).execute()
                            st.experimental_rerun()
        else:
            st.info("–ü–æ–∫–∞ –Ω–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.")
else:
    st.info("üîë –í–æ–π–¥–∏—Ç–µ –∏–ª–∏ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä.")
